{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wake county restaurant inspection project by Paige McKenzie\n",
    "Implements methods discussed in related [blog post]().\n",
    "\n",
    "Data courtesy of Wake County Open Data (pulled 7/3/19):\n",
    "* [Restaurants](https://data-wake.opendata.arcgis.com/datasets/restaurants-in-wake-county)\n",
    "* [Inspections](https://data-wake.opendata.arcgis.com/datasets/food-inspections)\n",
    "* [Violations](https://data-wake.opendata.arcgis.com/datasets/food-inspection-violations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data to Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = pd.read_csv('./data/Restaurants_in_Wake_County.csv', index_col=['OBJECTID'],\n",
    "                  parse_dates=['RESTAURANTOPENDATE'], infer_datetime_format=True)\n",
    "\n",
    "insp = pd.read_csv('./data/Food_Inspections.csv', index_col=['OBJECTID'],\n",
    "                  parse_dates=['DATE_'], infer_datetime_format=True)\n",
    "\n",
    "viol = pd.read_csv('./data/Food_Inspection_Violations.csv',\n",
    "                  parse_dates=['INSPECTDATE'], infer_datetime_format=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data exploration and cleaning\n",
    "### Restaurants dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([rest.dtypes.rename(\"Datatype\"),\n",
    "           rest.apply(pd.Series.nunique).rename(\"# of unique values\"),\n",
    "            rest.apply(pd.Series.isnull).mean().rename(\"% of missing values\")],\n",
    "          axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city is not standardized - lowercase, replace hyphen with space\n",
    "rest['CITY'] = rest['CITY'].str.lower().str.replace('-', ' ')\n",
    "\n",
    "# combine any 'CITY' value with less than 10 data points into an \"other\" category, treat nulls the same\n",
    "rest.loc[rest['CITY'].isin(rest['CITY'].value_counts()[rest['CITY'].value_counts()<10].index),\n",
    "        'CITY'] = 'other'\n",
    "rest['CITY'].fillna('other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postal code isn't standardized either - abbreviate to 5 digit format, treat as integer\n",
    "rest['POSTALCODE'] = rest['POSTALCODE'].apply(lambda st:int(st[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import findall\n",
    "\n",
    "# clean name by removing odd characters, lowercase (combining weird apostrophes)\n",
    "rest['NAME'] = rest['NAME'].str.lower().str.replace('`', \"'\").apply(lambda x:' '.join(findall(r\"([a-z'-]+)(?=\\s|$)\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no reason to retain the STATE field - it has no information, and all of Wake County is in NC anyway\n",
    "rest.drop('STATE', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([rest.dtypes.rename(\"Datatype\"),\n",
    "           rest.apply(pd.Series.nunique).rename(\"# of unique values\"),\n",
    "            rest.apply(pd.Series.isnull).mean().rename(\"% of missing values\")],\n",
    "          axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspections dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename date column to match other table\n",
    "insp.rename(columns={'DATE_':'INSPECTDATE'}, inplace=True)\n",
    "insp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insp['HSISID'].isin(rest['HSISID']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to connect inspections to restaurants, so we'll only retain rows where this is possible, about 97% of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insp = insp[insp['HSISID'].isin(rest['HSISID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([insp.dtypes.rename(\"Datatype\"),\n",
    "           insp.apply(pd.Series.nunique).rename(\"# of unique values\"),\n",
    "            insp.apply(pd.Series.isnull).mean().rename(\"% of missing values\")],\n",
    "          axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([viol.dtypes.rename(\"Datatype\"),\n",
    "           viol.apply(pd.Series.nunique).rename(\"# of unique values\"),\n",
    "            viol.apply(pd.Series.isnull).mean().rename(\"% of missing values\")],\n",
    "          axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data came without HSISID populated, but we may be able to crosswalk with the other tables to get this information. However, it is worth noting that the `violations` table has dramatically more unique `PERMITID` values than the other tables, meaning it will be impossible to connect these violations to permits/restaurants in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol['PERMITID'].isin(insp['PERMITID']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it looks like about 17% of the rows in the `violations` table can be connected to the others, so this is the subset we'll keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there is one HSISID for each PERMITID\n",
    "assert (rest.groupby('PERMITID')['HSISID'].nunique()>1).sum()==0\n",
    "\n",
    "viol = viol.drop('HSISID', axis=1).join(insp.set_index('PERMITID')['HSISID'].drop_duplicates(),\n",
    "         on='PERMITID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([viol.dtypes.rename(\"Datatype\"),\n",
    "           viol.apply(pd.Series.nunique).rename(\"# of unique values\"),\n",
    "            viol.apply(pd.Series.isnull).mean().rename(\"% of missing values\")],\n",
    "          axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've lost data, but at least everything that remains is usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rest.index.nunique(), \"restaurants\")\n",
    "print(len(viol), \"violations\")\n",
    "print(len(insp[['HSISID', 'INSPECTDATE']].drop_duplicates()), \"inspectors\")\n",
    "print(insp['INSPECTOR'].nunique(), \"inspectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date range:\\n{}\".format(insp['INSPECTDATE'].agg([min, max])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = insp.groupby(pd.Grouper(freq=\"M\", key='INSPECTDATE')).size()\n",
    "plt.plot(data, color='#31394d')\n",
    "plt.ylim((0,800))\n",
    "plt.title(\"Number of inspections conducted, per month\")\n",
    "plt.ylabel(\"# of inspections\")\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# calc the trendline\n",
    "z = np.polyfit(list(range(len(data))), data.values, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(data.index, p(list(range(len(data)))), color='#009384', linestyle='--')\n",
    "# the line equation:\n",
    "plt.figtext(x=.14, y=.14, s=\"y = %.3f * Months + %.3f\"%(z[0],z[1]), color='#009384')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = viol.groupby([pd.Grouper(freq=\"M\", key='INSPECTDATE'), viol['PERMITID']]).size().rename(\"VIOLCOUNT\").groupby(level=0).mean()\n",
    "plt.plot(data, color='#31394d')\n",
    "plt.ylim((0,12))\n",
    "plt.title(\"Average number of violations found in\\nan inspection, per month\")\n",
    "plt.ylabel(\"# of violations\")\n",
    "plt.xlabel(\"Date\")\n",
    "\n",
    "# calc the trendline\n",
    "z = np.polyfit(list(range(len(data))), data.values, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(data.index, p(list(range(len(data)))), color='#009384', linestyle='--')\n",
    "# the line equation:\n",
    "plt.figtext(x=.14, y=.14, s=\"y = %.3f * Months + %.3f\"%(z[0],z[1]), color='#009384')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insp.groupby('HSISID').size().div(insp.groupby('HSISID')['INSPECTDATE'].apply(lambda s:(s.max()-s.min()).days/365\n",
    "                                                                             if len(s)>1 else None)).dropna().hist(bins=30, color='#31394d')\n",
    "plt.title(\"Histogram of restaurants' # of inspections/year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Imagine it's Christmas break, end of 2018, and we've scheduled restaurant inspections for the first 6 months of 2019. In hopes of increasing the scores of at-risk restaurants, we'd like to implement a mailing campaign, targeted at restaurants who we expect to score low, giving them information about common violations and advice for proper procedures. Hopefully, this will elicit improvements before our inspector visits, and they will receive a higher inspection `SCORE`. \n",
    "\n",
    "Due to budget constraints, we can only mail to 500 restaurants in this campaign. We want to target the most \"at-risk\" customers using data science. For this, we want a model that can accurately predict which inspection is likely to receive a low `SCORE`, so we can target them.\n",
    "\n",
    "We define a \"low\" `SCORE` as `SCORE`<93."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_date = '2019-01-01'\n",
    "test_date = '2018-07-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "We will first engineer a set of features from what we know about each restaurant. This set will be split into a train and test set using a 70/30 split. Using appropriate models and grid search to select optimum parameters, we'll identify the best model using cross validation within the training set, and then the model that performs best on the reserved testing set will be selected for production.\n",
    "\n",
    "**Feature engineering:**\n",
    "\n",
    "Features will include:\n",
    "* the number of days the restaurant has been open (`TIMEOPEN`, integer)\n",
    "* the number of days since the restaurant was last inspected (`TIMESINCE`, integer)\n",
    "* the number of other restaurants (unique HSISIDs) with the same name (`CHAINCOUNT`, integer)\n",
    "* the number of inspections for the restaurant (`INSPCOUNT`, integer)\n",
    "* whether the restaurant has ever needed a re-inspection (`WASREINSP`, binary)\n",
    "* the average number of violations per inspection for that restaurant (`AVGVIOL`, float)\n",
    "\n",
    "Just for fun and in case of some seasonality to inspections, we'll throw in cyclical features for month:\n",
    "* `SINMONTH` (float)\n",
    "* `COSMONTH` (float)\n",
    "\n",
    "Including preexisting features:\n",
    "* `FACILITYTYPE` (categorical and will be converted to dummy variables)\n",
    "* `CITY` (categorical and will be converted to dummy variables)\n",
    "* `POSTALCODE` (integer, due to the fact that zipcodes that are close in number tend to be geographically similar as well)\n",
    "* `AREACODE` (categorical, will be extracted from the existing `PHONE NUMBER` field\n",
    "\n",
    "Wrapping in a few details on the restaurant's most recent inspection, prior to that in question:\n",
    "* the type of the restaurant's most recent inspection (`TYPE_previous`, categorical to dummy)\n",
    "* the inspector who completed the most recent inspection (`INSPECTOR_previous`, categorical to dummy)\n",
    "* the previous inspection score (`SCORE_previous`, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(insp, rest):\n",
    "    features = insp[['HSISID', 'INSPECTDATE', 'SCORE', 'INSPECTOR', 'TYPE']].join(\n",
    "        rest.set_index('HSISID')[['NAME', 'CITY', 'RESTAURANTOPENDATE', 'FACILITYTYPE', 'PHONENUMBER']], on='HSISID', how='left')\n",
    "\n",
    "    # calculate number of unique HSISID's with the same name (# of chain locations, maybe)\n",
    "    features = features.join((features.groupby('NAME')['HSISID'].nunique()).rename('CHAINCOUNT'), on='NAME')\n",
    "\n",
    "    # calculate number of inspections\n",
    "    features = features.join(insp.groupby('HSISID').size().rename(\"INSPCOUNT\").reindex(features['HSISID'].unique(), fill_value=0), on='HSISID')\n",
    "\n",
    "    # whether the restaurant ever needed a re-inspection\n",
    "    features['WASREINSP'] = features['HSISID'].isin(insp.loc[insp['TYPE']=='Re-Inspection', 'HSISID'].unique()).astype(int)\n",
    "\n",
    "    # average number of violations per inspection\n",
    "    features = features.join(insp.groupby(['HSISID', 'INSPECTDATE']).size().reindex(features[['HSISID', 'INSPECTDATE']], fill_value=0).mean(level=0).rename(\"AVGVIOL\"),\n",
    "                            on='HSISID')\n",
    "\n",
    "    # self-merge to get information about previous inspection\n",
    "    features = features.groupby('HSISID')[['SCORE', 'INSPECTOR', 'TYPE', 'INSPECTDATE']].apply(lambda group:\n",
    "                 pd.concat([group.add_suffix('_previous'),\n",
    "                            group.shift(-1).add_suffix('_current')],\n",
    "                           axis=1, sort=True)).drop(['INSPECTOR_current', 'TYPE_current'], axis=1).\\\n",
    "    rename(columns={'INSPECTDATE_current':'INSPECTDATE'}).\\\n",
    "    merge(features, on=['HSISID', 'INSPECTDATE'], how='right').drop_duplicates()\n",
    "\n",
    "\n",
    "    # time features\n",
    "    features['TIMEOPEN'] = (features['INSPECTDATE'] - features['RESTAURANTOPENDATE']).dt.days\n",
    "    features.loc[features['INSPECTDATE_previous'].isnull(), 'INSPECTDATE_previous'] = features['RESTAURANTOPENDATE']\n",
    "    features['TIMESINCE'] = (features['INSPECTDATE'] - features['INSPECTDATE_previous']).dt.days\n",
    "    from numpy import sin, cos\n",
    "    from math import pi\n",
    "    features['SINMONTH'] = features['INSPECTDATE'].dt.month.apply(lambda x:sin(2*pi*x/12))\n",
    "    features['COSMONTH'] = features['INSPECTDATE'].dt.month.apply(lambda x:cos(2*pi*x/12))\n",
    "\n",
    "    # derive area code feature as dummy columns\n",
    "    features = pd.concat([features, \n",
    "                          pd.get_dummies(features['PHONENUMBER'].str[1:4])],\n",
    "                         axis=1)\n",
    "\n",
    "    # drop extra columns\n",
    "    features = features.drop(['INSPECTOR', 'TYPE', 'RESTAURANTOPENDATE', 'INSPECTDATE_previous', 'SCORE_current', 'PHONENUMBER'], \n",
    "                             axis=1)\n",
    "    \n",
    "    # inpute missing values\n",
    "    features.loc[features['INSPECTOR_previous'].isnull(), 'INSPECTOR_previous'] = 'none' # simply add a new category\n",
    "    features.loc[features['TYPE_previous'].isnull(), 'TYPE_previous'] = 'none' # simply add a new category\n",
    "    features['NAME'].fillna('', inplace=True)\n",
    "    features['CHAINCOUNT'].fillna(1, inplace=True)\n",
    "    # fill previous score with moving average\n",
    "    features.loc[features['SCORE_previous'].isnull(), 'SCORE_previous'] = pd.merge_asof(features.sort_values('INSPECTDATE'), features.groupby(pd.Grouper(freq=\"M\", key='INSPECTDATE'))['SCORE'].mean().shift().to_frame().add_suffix('_fill'),\n",
    "              left_on='INSPECTDATE', right_index=True, direction='backward')['SCORE_fill']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-travel\n",
    "We will engineer features for training the models (all time, prior to `test_date`), testing the models (`test_date` to `validation_date`), and then see how our model would've done using the inspections after `validation_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = engineer_features(insp[insp['INSPECTDATE']<test_date], rest)\n",
    "\n",
    "validation = engineer_features(insp, rest)\n",
    "validation = validation[validation['INSPECTDATE']>=validation_date]\n",
    "\n",
    "test = engineer_features(insp[insp['INSPECTDATE']<validation_date], rest)\n",
    "test = test[test['INSPECTDATE']>=test_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For no reason at all, we will use NLP to parse the text in the restaurant's `NAME`, in hopes of extracting some insight into the cuisine type or food style (ie. `buffet` might indicate a higher chance of failing inspection). I've arbitrarily decided to choose the top 40 words from restaurant `NAME`, using most frequent words. Note that this will be \"trained\" using the training time period (`X`), and then applied to the prediction sets (`test`, `validation`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column size explodes from here as we add columns for words in the restaurant name\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# fit on training set\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=True, max_features=40)\n",
    "vectorizer.fit(X['NAME'])\n",
    "\n",
    "# apply to all sets\n",
    "X = pd.concat([X.drop('NAME', axis=1), \n",
    "       pd.DataFrame(vectorizer.transform(X['NAME']).todense().astype(int),\n",
    "         columns=vectorizer.vocabulary_, index=X.index)],\n",
    "      axis=1)\n",
    "validation = pd.concat([validation.drop('NAME', axis=1), \n",
    "       pd.DataFrame(vectorizer.transform(validation['NAME']).todense().astype(int),\n",
    "         columns=vectorizer.vocabulary_, index=validation.index)],\n",
    "      axis=1)\n",
    "test = pd.concat([test.drop('NAME', axis=1), \n",
    "       pd.DataFrame(vectorizer.transform(test['NAME']).todense().astype(int),\n",
    "         columns=vectorizer.vocabulary_, index=test.index)],\n",
    "      axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling:**\n",
    "\n",
    "We'll use cross-validation on the `X` dataset with known scores, to determine optimal model parameters. The reserved `test` set will be used to choose the best model, hopefully controlling for over-fitting. Finally, we will re-fit the model using the process we identify as best, and see how we would perform on the `validation` set, had we actually implemented the model-building process to identify the 500 most \"at-risk\" restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X['SCORE']<93).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's balance our classes, which should hopefully help the model separate the classes better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = 3 # the number of times to include each positive target row\n",
    "ratio = .5 # the goal target distribution (# of positive class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg = X[X['SCORE']<93]\n",
    "X_pos = X[X['SCORE']>=93]\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_pos.sample(frac=oversample, replace=True).append(\n",
    "    X_neg.sample(int((1/ratio-1)*oversample*len(X_pos)), replace=True))\n",
    "del X_neg, X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X['SCORE']<93).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully balanced the data to 50:50.\n",
    "\n",
    "Next, using all inspections prior to the `test_date` and after 2014, we'll implement cross-validation to choose optimal model parameters for a simple decision tree, bagging, boosting, and random forest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore inspections with missing vital data\n",
    "X = X[X['SCORE'].notna()&(X['INSPECTDATE'].dt.year>=2014)]\n",
    "\n",
    "y = (X['SCORE']<93).astype(int)\n",
    "X = X.drop(['SCORE', 'INSPECTDATE'], axis=1)\n",
    "\n",
    "# also separate test, validation into X, and y\n",
    "y_test = (test['SCORE']<93).astype(int)\n",
    "X_test = test.drop(['SCORE', 'INSPECTDATE'], axis=1)\n",
    "\n",
    "y_val = (validation['SCORE']<93).astype(int)\n",
    "X_val = validation.drop(['SCORE', 'INSPECTDATE'], axis=1)\n",
    "\n",
    "del test, validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a random prediction\n",
    "from numpy import zeros\n",
    "from sklearn.metrics import roc_auc_score\n",
    "naive_score = roc_auc_score(y, np.random.choice([0,1], size=len(y)))\n",
    "print(\"Random AUC:\", naive_score)\n",
    "print(\"Random lift on test set:\", y_test.loc[np.random.choice(y_test.index, size=500)].mean()/y_test.mean())\n",
    "# any models should perform better than this (have a larger AUC) to conclude that signal is being captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "grid = GridSearchCV(tree, param_grid={'max_depth':[3,5,7,12],\n",
    "                                     'max_features':[.5,.8,1.]},\n",
    "                   scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "tree = grid.best_estimator_\n",
    "y_pred = tree.predict(X_test)\n",
    "tree_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Single decision tree guessed {} inspections would fail, for AUC of {}\".format(y_pred.sum(), tree_score))\n",
    "if tree_score<naive_score:\n",
    "    print(\"This model surpassed a naive forecast w/ parameters\", grid.best_params_)\n",
    "export_graphviz(tree, out_file='tree.dot', feature_names=X_train.columns, impurity=False, label=None, proportion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bags = BaggingClassifier(random_state=1)\n",
    "grid = GridSearchCV(bags, param_grid={'n_estimators':[20,150],\n",
    "                                     'max_samples':[.5,.8]},\n",
    "                   scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "bags = grid.best_estimator_\n",
    "y_pred = bags.predict(X_test)\n",
    "bags_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Bagging guessed {} inspections would fail, for auc of {}\".format(y_pred.sum(), bags_score))\n",
    "if bags_score<naive_score:\n",
    "    print(\"This model surpassed a naive forecast w/ parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boost = GradientBoostingClassifier(random_state=1)\n",
    "grid = GridSearchCV(bags, param_grid={'n_estimators':[20,150],\n",
    "                                     'max_features':[.5,.8]},\n",
    "                   scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "boost = grid.best_estimator_\n",
    "y_pred = boost.predict(X_test)\n",
    "boost_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Boosting guessed {} inspections would fail, for auc of {}\".format(y_pred.sum(), boost_score))\n",
    "if boost_score<naive_score:\n",
    "    print(\"This model surpassed a naive forecast w/ parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "grid = GridSearchCV(bags, param_grid={'n_estimators':[200,350],\n",
    "                                     'max_samples':[.3,.5,.8],\n",
    "                                     'max_features':[.3,.5,.8]},\n",
    "                   scoring='roc_auc', cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "rf = grid.best_estimator_\n",
    "y_pred = rf.predict(X_test)\n",
    "rf_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest guessed {} inspections would fail, for auc of {}\".format(y_pred.sum(), rf_score))\n",
    "if rf_score<naive_score:\n",
    "    print(\"This model surpassed a naive forecast w/ parameters\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "The best out-of-sample model using log loss will be used to predict on the unknown inspections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
