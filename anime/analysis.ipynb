{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering project by Paige McKenzie\n",
    "\n",
    "Includes code to perform analysis discussed in my [blog post]().\n",
    "\n",
    "[Dataset](https://www.kaggle.com/azathoth42/myanimelist/version/9) available here.\n",
    "\n",
    "https://realpython.com/build-recommendation-engine-collaborative-filtering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "shows = pd.read_csv('anime_filtered.csv', index_col='anime_id', usecols=['title', 'anime_id'])\n",
    "reviews = pd.read_csv('animelists_filtered.csv', nrows=200000, usecols=['username', 'anime_id', 'my_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample to a complete set of reviews for a subset of shows\n",
    "reviews = reviews[reviews['anime_id'].isin(reviews['anime_id'].unique()[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot for one row per user, and column per anime\n",
    "reviews = pd.pivot_table(reviews, index='username', columns='anime_id', values='my_score', aggfunc=max)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our target shows\n",
    "target_cols = [210, 232, 233]\n",
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep users who have rated at least one target show\n",
    "# also only keep users who have rated at least one other show (users we have some information about)\n",
    "reviews = reviews.loc[reviews[target_cols].notna().max(axis=1) \n",
    "                      & (reviews.drop(target_cols, axis=1).notna().sum(axis=1)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[0], 'title']))\n",
    "plt.hist(reviews[target_cols[0]].dropna())\n",
    "plt.axvline(reviews[target_cols[0]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.subplot(312)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[1], 'title']))\n",
    "plt.hist(reviews[target_cols[1]].dropna())\n",
    "plt.axvline(reviews[target_cols[1]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.subplot(313)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[2], 'title']))\n",
    "plt.hist(reviews[target_cols[2]].dropna())\n",
    "plt.axvline(reviews[target_cols[2]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of people who bother to rate a show do so to assert their dislike of it (hense the spike at zero). Everybody else offered a little more granularity, with most people really liking it (a score of 10) with a tail towards zero. Clearly, we would never want to recommend this show to someone who we think is going to hate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll split the dataset into two groups:\n",
    "\n",
    "1. Train (the \"known\") - users who have scored the show we'll recommend, and whose scores we'll use to model\n",
    "2. Test (the \"unknown\") - users who have scored the show we'll recommend, but whose scores we'll ignore and only use at the end, for measuring how well we targeted the subset that would enjoy the show\n",
    "\n",
    "Our goal for this project is to successfully predict how an unknown user would rate a new show, given their existing watching preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=.3, random_state=1)\n",
    "\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# baseline (median)\n",
    "baselines = []\n",
    "\n",
    "for target_col in target_cols:\n",
    "    score = mean_absolute_error(test[target_col].dropna(),\n",
    "                    np.repeat(train[target_col].median(), test[target_col].notna().sum()))\n",
    "    baselines.append(score)\n",
    "    print(\"Error in scores when recommending '{}' (median baseline): {}\".format(shows.loc[target_col, 'title'],\n",
    "        round(score, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train[target_cols]\n",
    "test_targets = test[target_cols]\n",
    "\n",
    "train = train.drop(target_cols, axis=1)\n",
    "test = test.drop(target_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center reviews, saving the average per user\n",
    "train_med = train.median(axis=1)\n",
    "test_med = test.median(axis=1)\n",
    "\n",
    "train = train.apply(lambda col:col-train_med)\n",
    "test = test.apply(lambda col:col-test_med)\n",
    "\n",
    "train_targets = train_targets.apply(lambda col:col-train_med)\n",
    "test_targets = test_targets.apply(lambda col:col-test_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find inter-user similarity (ignoring our target columns)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = pd.DataFrame(cosine_similarity(train.fillna(0), test.fillna(0)), \n",
    "                   index=train.index, columns=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one row per known user, one column per unknown user\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_col, baseline in zip(target_cols, baselines):\n",
    "    # actual ratings for first target show (for those users in the test set who rated it)\n",
    "    actual = (test_targets[target_col]+test_med).dropna()\n",
    "    \n",
    "    # get the most similar user's adjustment on this show, \n",
    "    ## then apply that adjustment to the unknown user's avg score\n",
    "    pred_single = sim.loc[train_targets[target_col].notna(), actual.index].apply(lambda col:\n",
    "                          train_targets.loc[col.idxmax(), target_col] if col.max()>0 else 0)+test_med.reindex(actual.index)\n",
    "    \n",
    "    pred_multiple = sim.loc[train_targets[target_col].notna(), actual.index].apply(lambda col:\n",
    "                          train_targets.loc[col.nlargest(15).index, target_col][col.nlargest(15)>0].median()).fillna(0)+test_med.reindex(actual.index)\n",
    "    \n",
    "    # adjust impossible scores\n",
    "    pred_single[pred_single<0] = 0\n",
    "    pred_single[pred_single>10] = 10\n",
    "    pred_multiple[pred_multiple<0] = 0\n",
    "    pred_multiple[pred_multiple>10] = 10\n",
    "\n",
    "    score_single = mean_absolute_error(actual,\n",
    "                                pred_single)\n",
    "    score_multiple = mean_absolute_error(actual,\n",
    "                                pred_multiple)\n",
    "    \n",
    "    print(\"Error in scores when recommending '{0}' (single most similar user): {1}, decreasing the baseline error by {2:.1%}\".format(shows.loc[target_col, 'title'],\n",
    "        round(score_single, 2),\n",
    "        (baseline-score_single)/baseline))\n",
    "    print(\"Error in scores when recommending '{0}' (25 most similar users): {1}, decreasing the baseline error by {2:.1%}\".format(shows.loc[target_col, 'title'],\n",
    "        round(score_multiple, 2),\n",
    "        (baseline-score_multiple)/baseline))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, it looks like the wisdom of the herd does really well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The true power of Collaborative Filtering\n",
    "\n",
    "Dataset reduction! What if we only needed to \"remember\" a handful of user's preferences, rather than all of them, in order to still predict how interested a user will be in a show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0).groupby(train.columns.tolist()).size().rename('weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cosine_similarity(train.head(100).fillna(0)), \n",
    "                       index=train.head(100).index, columns=train.head(100).index)\n",
    "np.fill_diagonal(X_train.values, 0)\n",
    "\n",
    "metrics = X_train.agg({max, pd.Series.idxmax}).T\n",
    "metrics = metrics[metrics['max']==1]\n",
    "vals = metrics['idxmax'].value_counts()\n",
    "keepers = vals[vals>1].index.tolist()\n",
    "#keepers += metrics.loc[~metrics['idxmax'].isin(keepers), 'idxmax'].tolist()\n",
    "keepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[~(metrics.index.isin(keepers) | metrics['idxmax'].isin(keepers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary = metrics['idxmax'].unique()\n",
    "\n",
    "metrics.reindex(necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc[metrics['max']==1, 'idxmax'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[(metrics['max']==1) & (metrics['idxmax']=='KatieMH')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(['max']==1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mod = KMeans(n_clusters=15)\n",
    "\n",
    "mod.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('users_filtered.csv', index_col='username')\n",
    "users = users[users.index.isin(reviews.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_mod = LinearRegression()\n",
    "\n",
    "lin_mod.fit(train.drop(target_col, axis=1).apply(lambda col:col.fillna(col.median())).values, train[target_col])\n",
    "\n",
    "lin_pred = lin_mod.predict(test.drop(target_col, axis=1).apply(lambda col:col.fillna(train[col.name].median())).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark purely random guess\n",
    "mean = pd.Series(lin_pred, index=test.index).sort_values(ascending=False).head(int(len(test)*target_frac)).mean()\n",
    "print(\"Linear regression achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"When user gives at least one show a 0 rating\")\n",
    "plt.xlabel('Average scores given')\n",
    "plt.hist((train.loc[(train==0).max(axis=1)]==0).mean(axis=1))\n",
    "\n",
    "pyplot.subplot(121)\n",
    "plt.title(\"In general\")\n",
    "plt.xlabel('Average scores given')\n",
    "plt.hist(train.mean(axis=1))\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_mod = LogisticRegression()\n",
    "\n",
    "log_mod.fit(train.drop(target_col, axis=1).apply(lambda col:col.fillna(col.median())).values, train[target_col]>0)\n",
    "\n",
    "log_pred = log_mod.predict_proba(test.drop(target_col, axis=1).apply(lambda col:col.fillna(train[col.name].median())).values)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark purely random guess\n",
    "mean = test.loc[pd.Series(log_pred, index=test.index).sort_values(ascending=False).head(int(len(test)*target_frac)).index, target_col].mean()\n",
    "print(\"Logistic regression achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#store = {}\n",
    "\n",
    "overall = pd.Series(index=test.index)\n",
    "\n",
    "for i in range(1,len(train.columns)):\n",
    "    for combine in combinations(train.columns.drop(target_col), i):\n",
    "        #print(combine, sum(~train[list(combine)].isna().max(axis=1)))\n",
    "        #store[combine] = train.loc[~train[list(combine)].isna().max(axis=1), target_col].mean()\n",
    "        \n",
    "        train_subset = train.loc[~train[list(combine)].isna().max(axis=1)]\n",
    "        \n",
    "        train_subset.groupby(train_subset.columns.drop(target_col))\n",
    "        test_subset = test.loc[~test[list(combine)].isna().max(axis=1)]\n",
    "\n",
    "        users = pd.Series(train.loc[pd.DataFrame(pairwise_distances(train_subset, test_subset, metric='euclidean'), \n",
    "                                         index=train_subset.index,\n",
    "                                            columns=test_subset.index).idxmin().values, \n",
    "                                    target_col].values,\n",
    "                          index=test_subset.index)\n",
    "        overall.loc[users.index] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset.groupby(list(train.columns.drop(target_col)))[target_col].mean().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark collaborative filtering\n",
    "mean = test.loc[overall.sort_values(ascending=False).head(int(len(test)*target_frac)).index,\n",
    "         target_col].mean()\n",
    "print(\"Collaborative filtering achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
