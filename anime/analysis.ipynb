{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering project by Paige McKenzie\n",
    "\n",
    "Includes code to perform analysis discussed in my [blog post]().\n",
    "\n",
    "[Dataset](https://www.kaggle.com/azathoth42/myanimelist/version/9) available here.\n",
    "\n",
    "https://realpython.com/build-recommendation-engine-collaborative-filtering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "shows = pd.read_csv('anime_filtered.csv', index_col='anime_id', usecols=['title', 'anime_id'])\n",
    "reviews = pd.read_csv('animelists_filtered.csv', nrows=400000, usecols=['username', 'anime_id', 'my_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample to a complete set of reviews for a subset of shows\n",
    "reviews = reviews[reviews['anime_id'].isin(reviews['anime_id'].unique()[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot for one row per user, and column per anime\n",
    "reviews = pd.pivot_table(reviews, index='username', columns='anime_id', values='my_score', aggfunc=max)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our target show\n",
    "target_cols = reviews.columns[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep users who have rated at least one target show\n",
    "# also only keep users who have rated at least one other show (users we have some information about)\n",
    "reviews = reviews.loc[reviews[target_cols].notna().max(axis=1) \n",
    "                      & (reviews.drop(target_cols, axis=1).notna().sum(axis=1)>0)]\n",
    "\n",
    "# sample for data scale\n",
    "reviews = reviews.sample(frac=.6)\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[0], 'title']))\n",
    "plt.hist(reviews[target_cols[0]].dropna())\n",
    "plt.axvline(reviews[target_cols[0]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.subplot(312)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[1], 'title']))\n",
    "plt.hist(reviews[target_cols[1]].dropna())\n",
    "plt.axvline(reviews[target_cols[1]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.subplot(313)\n",
    "plt.title(\"Distribution of reviews for '{}'\".format(shows.loc[target_cols[2], 'title']))\n",
    "plt.hist(reviews[target_cols[2]].dropna())\n",
    "plt.axvline(reviews[target_cols[2]].mean(), color='purple', ls='--')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of people who bother to rate a show do so to assert their dislike of it (hense the spike at zero). Everybody else offered a little more granularity, with most people really liking it (a score of 10) with a tail towards zero. Clearly, we would never want to recommend this show to someone who we think is going to hate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll split the dataset into two groups:\n",
    "\n",
    "1. Train (the \"known\") - users who have scored the show we'll recommend, and whose scores we'll use to model\n",
    "2. Test (the \"unknown\") - users who have scored the show we'll recommend, but whose scores we'll ignore and only use at the end, for measuring how well we targeted the subset that would enjoy the show\n",
    "\n",
    "Our goal for this project is to recommend each show to some of the unknown users, with the goal of recommending each show to the subset of users that will enjoy the show most (rate it highest).\n",
    "\n",
    "Let's also assume we will only suggest the show to the top `target_frac` of unknown users (in this case, 20%), factoring in how popular the show is (based on how many users in the train set have rated it). More succinctly, we will recommend the show to more people if the show has a lot of ratings in the train set, and recommend to fewer people if it doesn't have as many ratings, but our goal of targeting the top 20% will stay the same despite the show's viewership numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(reviews, test_size=.3, random_state=1)\n",
    "\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frac = 0.2\n",
    "\n",
    "target_sizes = [int(len(test)*train[target_col].notna().mean()*target_frac) for target_col in target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline (just random sample)\n",
    "baselines = []\n",
    "\n",
    "for target_col, size in zip(target_cols, target_sizes):\n",
    "    score = test.loc[test[target_col].notna(), target_col].sample(size, random_state=1).mean()\n",
    "    baselines.append(score)\n",
    "    print(\"Average rating (random sample) when recommending '{}' to {} users: {}\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline (sort by average rating user gives other shows)\n",
    "\n",
    "for target_col, size, baseline in zip(target_cols, target_sizes, baselines):\n",
    "    score = test.loc[test.drop(target_col, axis=1).mean(axis=1)[test[target_col].notna()].sort_values(ascending=False).head(size).index,\n",
    "                        target_col].mean()\n",
    "    print(\"Average rating (highest user-average on other shows) when recommending '{}' to {} users: {}\\n\\t- Lift of {} over baseline\\n\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2),\n",
    "        round(score/baseline, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we've really managed to do here is target \"happier\" users, or perhaps those with lower standards - however, it has produced a noticeable lift in the scores we get when recommending each show. Can we do better, by using each user's reviews on other shows as a basis for their enjoyment of these shows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train[target_cols]\n",
    "test_targets = test[target_cols]\n",
    "\n",
    "train = train.drop(target_cols, axis=1)\n",
    "test = test.drop(target_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-center reviews, saving the average per user\n",
    "train_avg = train.mean(axis=1)\n",
    "test_avg = test.mean(axis=1)\n",
    "\n",
    "train = train.apply(lambda col:col-train_avg)\n",
    "test = test.apply(lambda col:col-test_avg)\n",
    "\n",
    "train_targets = train_targets.apply(lambda col:col-train_avg)\n",
    "test_targets = test_targets.apply(lambda col:col-test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find inter-user similarity (ignoring our target columns)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim = pd.DataFrame(cosine_similarity(train.fillna(0), test.fillna(0)), \n",
    "                   index=train.index, columns=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one row per known user, one column per unknown user\n",
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_user(col):\n",
    "    # find most similar known user\n",
    "    user = col.idxmax()\n",
    "    \n",
    "    # return that user's adjustment for this show\n",
    "    ## if user is similar at all\n",
    "    return train_targets.loc[user, target_col] if col.max()>0 else 0\n",
    "    \n",
    "for target_col, size, baseline in zip(target_cols, target_sizes, baselines):\n",
    "    # actual ratings for first target show (for those users in the test set who rated it)\n",
    "    actual = (test_targets[target_col]+test_avg).dropna()\n",
    "    \n",
    "    # get the most similar user's adjustment on this show, \n",
    "    ## then apply that adjustment to the unknown user's avg score\n",
    "    pred = sim.loc[train_targets[target_col].notna(), actual.index].apply(most_similar_user)+test_avg.reindex(actual.index)\n",
    "    # adjust impossible scores\n",
    "    pred[pred<0] = 0\n",
    "    pred[pred>10] = 10\n",
    "\n",
    "    score = actual[pred.sort_values(ascending=False).head(size).index].mean()\n",
    "    \n",
    "    print(\"Average rating (single most-similar user) when recommending '{}' to {} users: {}\\n\\t- Lift of {} over baseline\\n\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2),\n",
    "        round(score/baseline, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but not as good as simply targeting \"happy\" users - perhaps because we're only using a single \"most similar\" user right now. Let's expand our criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_similar_users(col, n=10):\n",
    "    # find most similar known user\n",
    "    users = col.sort_values(ascending=False).head(n).index\n",
    "    users = users[col[users]>0]\n",
    "    \n",
    "    if len(users)==0:\n",
    "        # no similar users, no adjustment\n",
    "        return 0\n",
    "    \n",
    "    # return the average adjustment across all similar users for this show\n",
    "    return train_targets.loc[users, target_col].mean()\n",
    "    \n",
    "for target_col, size, baseline in zip(target_cols, target_sizes, baselines):\n",
    "    # actual ratings for first target show (for those users in the test set who rated it)\n",
    "    actual = (test_targets[target_col]+test_avg).dropna()\n",
    "    \n",
    "    # get the most similar users' adjustments on this show, \n",
    "    ## then apply that adjustment to the unknown user's avg score\n",
    "    pred = sim.loc[train_targets[target_col].notna(), actual.index].apply(n_most_similar_users)+test_avg.reindex(actual.index)\n",
    "    # adjust impossible scores\n",
    "    pred[pred<0] = 0\n",
    "    pred[pred>10] = 10\n",
    "\n",
    "    score = actual[pred.sort_values(ascending=False).head(size).index].mean()\n",
    "    \n",
    "    print(\"Average rating (n=10 most-similar users) when recommending '{}' to {} users: {}\\n\\t- Lift of {} over baseline\\n\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2),\n",
    "        round(score/baseline, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wisdom of the herd!** You love to see it.\n",
    "\n",
    "So what's next? Right now we're factoring in only the n-most similar users. But could it be possible that we could learn just as much from the most *dissimilar* users and moving in the opposite direction? Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_similar_user(col):\n",
    "    # find most similar known user\n",
    "    user = col.idxmin()\n",
    "    \n",
    "    # return that user's adjustment for this show\n",
    "    ## if user is similar at all\n",
    "    return train_targets.loc[user, target_col] if col.min()<0 else 0\n",
    "    \n",
    "for target_col, size, baseline in zip(target_cols, target_sizes, baselines):\n",
    "    # actual ratings for first target show (for those users in the test set who rated it)\n",
    "    actual = (test_targets[target_col]+test_avg).dropna()\n",
    "    \n",
    "    # get the OPPOSITE of the LEAST similar user's adjustment on this show, \n",
    "    ## then apply that adjustment to the unknown user's avg score\n",
    "    pred = -1*sim.loc[train_targets[target_col].notna(), actual.index].apply(most_similar_user)+test_avg.reindex(actual.index)\n",
    "    # adjust impossible scores\n",
    "    pred[pred<0] = 0\n",
    "    pred[pred>10] = 10\n",
    "\n",
    "    score = actual[pred.sort_values(ascending=False).head(size).index].mean()\n",
    "    \n",
    "    print(\"Average rating (single least-similar user) when recommending '{}' to {} users: {}\\n\\t- Lift of {} over baseline\\n\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2),\n",
    "        round(score/baseline, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as the most similar user method, but interestingly still some good information here. Our intuition was right. Let's see if the herd mentality works here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_least_similar_users(col, n=10):\n",
    "    # find most similar known user\n",
    "    users = col.sort_values(ascending=True).head(n).index\n",
    "    users = users[col[users]<0]\n",
    "    \n",
    "    if len(users)==0:\n",
    "        # no similar users, no adjustment\n",
    "        return 0\n",
    "    \n",
    "    # return the average adjustment across all similar users for this show\n",
    "    return train_targets.loc[users, target_col].mean()\n",
    "    \n",
    "for target_col, size, baseline in zip(target_cols, target_sizes, baselines):\n",
    "    # actual ratings for first target show (for those users in the test set who rated it)\n",
    "    actual = (test_targets[target_col]+test_avg).dropna()\n",
    "    \n",
    "    # get the OPPOSITE of the LEAST similar users adjustments on this show, \n",
    "    ## then apply that adjustment to the unknown user's avg score\n",
    "    pred = -1*sim.loc[train_targets[target_col].notna(), actual.index].apply(n_most_similar_users)+test_avg.reindex(actual.index)\n",
    "    # adjust impossible scores\n",
    "    pred[pred<0] = 0\n",
    "    pred[pred>10] = 10\n",
    "\n",
    "    score = actual[pred.sort_values(ascending=False).head(size).index].mean()\n",
    "    \n",
    "    print(\"Average rating (n=10 least-similar users) when recommending '{}' to {} users: {}\\n\\t- Lift of {} over baseline\\n\".format(shows.loc[target_col, 'title'],\n",
    "        size,\n",
    "        round(score, 2),\n",
    "        round(score/baseline, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost as good as the herd approach for similar users! \n",
    "\n",
    "What else can we try? Maybe a weighted average, rather than a straight average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_similarity(col, qnt):\n",
    "    sim_subset = col.sort_values(ascending=False).iloc[1:qnt+1]\n",
    "    return (views[target_col].reindex(sim_subset.index)*sim_subset).sum()/sim_subset.sum()\n",
    "\n",
    "rating = sim.apply(lambda col:weighted_similarity(col, qnt=10)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(mean_squared_error((views[target_col]+avg_ratings)[views[target_col].notna()], \n",
    "                        (rating+avg_ratings)[views[target_col].notna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('users_filtered.csv', index_col='username')\n",
    "users = users[users.index.isin(reviews.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_mod = LinearRegression()\n",
    "\n",
    "lin_mod.fit(train.drop(target_col, axis=1).apply(lambda col:col.fillna(col.median())).values, train[target_col])\n",
    "\n",
    "lin_pred = lin_mod.predict(test.drop(target_col, axis=1).apply(lambda col:col.fillna(train[col.name].median())).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark purely random guess\n",
    "mean = pd.Series(lin_pred, index=test.index).sort_values(ascending=False).head(int(len(test)*target_frac)).mean()\n",
    "print(\"Linear regression achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3.5))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"When user gives at least one show a 0 rating\")\n",
    "plt.xlabel('Average scores given')\n",
    "plt.hist((train.loc[(train==0).max(axis=1)]==0).mean(axis=1))\n",
    "\n",
    "pyplot.subplot(121)\n",
    "plt.title(\"In general\")\n",
    "plt.xlabel('Average scores given')\n",
    "plt.hist(train.mean(axis=1))\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_mod = LogisticRegression()\n",
    "\n",
    "log_mod.fit(train.drop(target_col, axis=1).apply(lambda col:col.fillna(col.median())).values, train[target_col]>0)\n",
    "\n",
    "log_pred = log_mod.predict_proba(test.drop(target_col, axis=1).apply(lambda col:col.fillna(train[col.name].median())).values)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark purely random guess\n",
    "mean = test.loc[pd.Series(log_pred, index=test.index).sort_values(ascending=False).head(int(len(test)*target_frac)).index, target_col].mean()\n",
    "print(\"Logistic regression achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "#store = {}\n",
    "\n",
    "overall = pd.Series(index=test.index)\n",
    "\n",
    "for i in range(1,len(train.columns)):\n",
    "    for combine in combinations(train.columns.drop(target_col), i):\n",
    "        #print(combine, sum(~train[list(combine)].isna().max(axis=1)))\n",
    "        #store[combine] = train.loc[~train[list(combine)].isna().max(axis=1), target_col].mean()\n",
    "        \n",
    "        train_subset = train.loc[~train[list(combine)].isna().max(axis=1)]\n",
    "        \n",
    "        train_subset.groupby(train_subset.columns.drop(target_col))\n",
    "        test_subset = test.loc[~test[list(combine)].isna().max(axis=1)]\n",
    "\n",
    "        users = pd.Series(train.loc[pd.DataFrame(pairwise_distances(train_subset, test_subset, metric='euclidean'), \n",
    "                                         index=train_subset.index,\n",
    "                                            columns=test_subset.index).idxmin().values, \n",
    "                                    target_col].values,\n",
    "                          index=test_subset.index)\n",
    "        overall.loc[users.index] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset.groupby(list(train.columns.drop(target_col)))[target_col].mean().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark collaborative filtering\n",
    "mean = test.loc[overall.sort_values(ascending=False).head(int(len(test)*target_frac)).index,\n",
    "         target_col].mean()\n",
    "print(\"Collaborative filtering achieves an average score of {} for a lift of {}\".format(round(mean, 2), \n",
    "                                                                                round(mean/train[target_col].mean(), 2)))\n",
    "del mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
